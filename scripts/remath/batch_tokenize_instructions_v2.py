"""
generate tokens from instructions.txt file that is generated by Ghidra
we need to extract tokens per function
we need to differentiate between function parameters and local variables
see: batch_tokenize_instructions.py for the previous version of the tokenizer
Follows the hierarchical nature of the program
1] program: object representation of instructions.txt file
2] program has multiple functions
3] function has metadata and multiple instructions
4] each instruction has opcode and operands
5] each function has key-value pairs for:
    value tokens - local variables
    param tokens - function parameters / arguments
    address tokens - address (memory address / registers)
"""

import argparse
import pathlib
import os
from typing import List, Any, Dict
from collections import OrderedDict


# util functions for tokenization
def parse_unicode_string_list(ustr: str):
    """
    extract unicode string literal from given unicode string
    """
    chunks = list()
    in_str = False
    str_start = None
    for i in range(1, len(ustr)):
        if ustr[i - 1] == 'u' and ustr[i] == '\'' and not in_str:
            in_str = True
            str_start = i + 1
        elif ustr[i - 1] != '\\' and ustr[i] == '\'' and in_str:
            in_str = False
            chunks.append(ustr[str_start: i])
    return chunks


def parse_metadata(metadata: str):
    """
    parse metadata for each instruction (if any)
    returns: parsed metadata for array and unparsed metadata for others
    """
    if metadata.startswith('array'):
        parsed_metadata = [':array']
        t, rest = metadata[6:-1].split(',', maxsplit=1)
        parsed_metadata.append(t)

        elms = parse_unicode_string_list(rest.strip(' []'))
        for elm in elms:
            parsed_metadata.append(elm)

        return parsed_metadata
    else:
        return [':unparsed', metadata]


def parse_instruction_line(line):
    """
    parse each instruction line into opcode, operands, and metadata
    return opcode, operands, metadata
    """
    operands: List[str] = []
    metadata: str = ''
    if '>>>' in line:
        first, rest = line.split('>>>')
        line = first.strip(' ')
        metadata = rest.strip(' ')
    line = line.split(' ', maxsplit=1)
    opcode = line[0]
    if len(line) > 1:
        if ',' in line[1]:
            args = line[1].split(',')
            for arg in args:
                operands.append(arg)
        else:
            operands.append(line[1])
    return opcode, operands, metadata


def parse_fn_name_from_parsed_metadata(parsed_metadata: List):
    if parsed_metadata[0] == ':array' and len(parsed_metadata) == 3:
        target_string = parsed_metadata[2]
        if '(' in target_string:
            fn_name = target_string.split('(')[0].split(' ')[-1]
            return ':fn_name', fn_name
        else:
            raise Exception(f"ERROR parse_fn_name_from_parsed_metadata()\n"
                            f"target_string does not appear to contain a function signature"
                            f"target_string: {target_string}"
                            f"parsed_metadata: {parsed_metadata}")
    else:
        raise Exception(f"ERROR parse_fn_name_from_parsed_metadata()\n"
                        f"parsed_metadata is not an array of length 3:\n"
                        f"parsed_metadata: {parsed_metadata}")


def parse_string_value_from_parsed_metadata(metadata):
    if len(metadata) == 3 and metadata[0] == ':array' and metadata[1] == 'java.lang.String':
        return ':string', metadata[2]
    else:
        return None


def parse_hex_value(value):
    """
    translate hex to decimal using 2's complement
    translate hex value ---------> binary -------------> decimal
    hex to binary: fixed mapping
    binary to decimal: 2's complement
    """
    # need a dictionary that maps every float number into 4 bit binary number
    hex_to_bin = {"0": "0000", "1": "0001", "2": "0010", "3": "0011", "4": "0100",
                  "5": "0101", "6": "0110", "7": "0111", "8": "1000", "9": "1001",
                  "a": "1010", "b": "1011", "c": "1100", "d": "1101", "e": "1110",
                  "f": "1111"}
    result = ""
    # remove the initial 0x if it exists
    if value.startswith("-0x"):
        value = value[3:]
        result = -(int(value, 16))
        return ':interpreted_hex', value, result
    elif value.startswith("0x"):
        value = value[2:]
    # convert to lower case
    value = value.lower()
    # for each digit convert it to binary
    for item in value:
        result += hex_to_bin[item]
    # if the initial value is zero: it's a positive number
    if result[0] == "0":
        # convert it to integer
        result = int(result, 2)
    # else the number is negative, convert it differently
    else:
        # flip the bits
        temp = ''.join(['1' if i == '0' else '0' for i in result])
        # add 1 and convert to decimal => convert to decimal and add 1
        result = -(int(temp, 2) + 1)
    return ':interpreted_hex', value, result


class Stack:
    """
    class Stack: used to implement call stack of functions called from main function
    each program has an instance of Stack to keep track of the functions being called by the
    main function
    """

    def __init__(self):
        self.stack = []

    def push(self, item):
        self.stack.append(item)

    def pop(self):
        return self.stack.pop()

    def is_empty(self):
        return len(self.stack) == 0

    def len(self):
        return len(self.stack)


class Tokens:
    """
    class that represents key-value pair for different tokens (value, param, address)
    """

    def __init__(self, name: str):
        """
        name of token: value, param, address
        counter: start value for each token
        token_to_elm: ordered dict for token to element
        """
        self.name = name
        self.base = "_" + name.lower()[0]
        self.counter = 0
        # key value pair from key to value
        self.token_to_elm: Dict[str, str] = OrderedDict()
        # reverse dictionary of self.elm_to_token: for efficient search to find if elm exists already
        self.elm_to_token: Dict[str, str] = dict()

    def add_token(self, elm):
        """
        elm: element to add to the token dict
        return: the key to which the elm was assigned to or the previous key if
        the element already exists: same id for repeating tokens
        """
        # if the elm already exists in the dict, just return the key of that element
        if elm in self.elm_to_token:
            return self.elm_to_token[elm]
        # if not assign the elm to new key and return new key
        key = self.base + str(self.counter)
        # update key-elm dict
        self.token_to_elm[key] = elm
        # update elm-key dict
        self.elm_to_token[elm] = key
        self.counter += 1
        return key

    def __str__(self):
        """
        string representaton of the key value pairs
        to write them to a __tokens.txt file
        """
        return ''.join(str(key) + ':' + str(value) + '\n' for key, value in self.token_to_elm.items())


class Program:
    """
    Program class: holds multiple functions: equivalent to the object representation of
    given instructions.txt file
    """

    def __init__(self):
        # file_name: name of the file that represents the program
        self.name: str = ''
        # dict of functions [address:function]
        # [name:function] is not valid because we can have multiple functions with the same name [printf]
        # this means we should use address for call navigation
        self.functions: Dict[str, Function] = dict()
        # book-keeping for unique functions [name:address]
        self.name_to_address: Dict[str, str] = dict()
        # initialize it's call stack of functions being called from it's main function
        self.stack = Stack()

    def update_name(self, name):
        self.name = name

    def add_function(self, function):
        if function.address in self.functions:
            raise Exception(f"[ERROR]: function with address: {function.address} already exists!")
        self.functions[function.address] = function
        # add book-keeping
        if function.name not in self.name_to_address:
            self.name_to_address[function.name] = function.address

    def get_function_by_name(self, name):
        if name not in self.name_to_address:
            raise Exception(f"[ERROR]: unable to find function with name: {name}")
        address = self.name_to_address[name]
        return self.functions[address]


class Function:
    """
    Function class: holds the data for each function in the instructions.txt file
    Program has multiple functions
    """

    def __init__(self, address, name, min_address, max_address):
        self.name: str = name
        self.address: str = address
        self.min_address: str = min_address
        self.max_address: str = max_address
        # raw lines for the function [not tokenized yet]
        self.lines: List[str] = list()
        # tokenized version of lines: List of instructions
        self.instructions: List[Instruction] = list()
        # list of tokens to be used for NMT
        self.tokens_nmt = list()
        # each function has key-value pairs for value tokens, param tokens, and address tokens
        self.value_tokens = Tokens("value")
        self.param_tokens = Tokens("param")
        self.address_tokens = Tokens("address")
        # list of functions called by this function
        self.called_fns = []

    def add_lines(self, lines):
        """
        add raw lines for the given function
        """
        self.lines.extend(lines)

    def tokenize_function(self):
        """
        convert the raw lines (self.lines) into Instruction
        and call tokenize instructions that tokenizes them and updates tokens_nmt
        also return a list of functions called by this function
        """
        # list of functions called by this function
        functions_called = list()
        for line in self.lines:
            addr_str, rest = line.split('::')
            addr = addr_str.strip(' ').split('x')[1]
            opcode, operands, metadata = parse_instruction_line(rest.strip(' '))
            instruction = Instruction(addr, opcode, operands, metadata)
            self.instructions.append(instruction)
        # a function has multiple instruction
        # call tokenize_instruction for each instruction to tokenize them all
        for instruction in self.instructions:
            # each instruction needs to be tokenized according to it's type
            # call instruction.tokenize() that detects the type and tokenizes accordingly
            instruction_tokens = instruction.tokenize_instruction()
            # each instruction_token is a tuple
            # tuple[0]: type of token: opcode, value, address, param, function(name of function)
            # tuple[1]: actual token

            # list of tokens for nmt for each instruction
            nmt_tokens_instruction = list()
            # keep track of current function name: to find if it's printf or not
            current_fn_name = ''
            for instruction_token in instruction_tokens:
                token_type, token = instruction_token
                if token_type == "value":
                    key = self.value_tokens.add_token(token)
                    nmt_tokens_instruction.append(key)
                elif token_type == "address":
                    key = self.address_tokens.add_token(token)
                    nmt_tokens_instruction.append(key)
                elif token_type == 'function_name':
                    current_fn_name = token
                    nmt_tokens_instruction.append(token)
                elif token_type == 'function_address':
                    if current_fn_name != 'printf':
                        # append the address only without 0x
                        functions_called.append(token[2:])
                else:
                    nmt_tokens_instruction.append(token)

            # add the tokens of a line (instructon) to tokens_nmt
            self.tokens_nmt.extend(nmt_tokens_instruction)
        return functions_called


class Instruction:
    """
    Instruction class: each line inside the function in the instructions.txt file
    Function has multiple instructions
    """

    def __init__(self, address, opcode, operands, metadata):
        """
        each instruction has address [in memory], and tuple (opcode, operands)
        some instructions have metadata
        parent is the function this instruction belongs to
        """
        self.address: str = address
        self.opcode: str = opcode
        self.operands: List[Any] = operands
        self.metadata = metadata
        self.parsed_metadata = []

    def tokenize_function_call_instruction(self):
        """
        tokenize operands for function call (with metadata / no metadata)
        returns a list of tuples
        """
        # return list is a tuple
        # tuple [0]: function_name, function_address
        # tuple[1]: actual token
        return_list = []
        if self.parsed_metadata:
            fn_name = parse_fn_name_from_parsed_metadata(self.parsed_metadata)
            # also return the address of the function: because it's uinque instead of names
            return_list.append(("function_name", fn_name[1]))
        else:
            raise Exception(f"Unable to get function name from metadata!")
        # call followed by 0xaddress, gives the address of the function
        if self.opcode == 'CALL' and self.operands[0].startswith('0x'):
            return_list.append(("function_address", self.operands[0]))

        return return_list

    def tokenize_normal_instruction(self):
        """
        tokenize normal operands (with metadata / no metadata)
        """
        tokens = list()
        for operand in self.operands:
            # value tokens
            if operand.startswith('0x') or operand.startswith('-0x'):
                if self.parsed_metadata:
                    # matches following sample
                    # opcode reg, 0xnum
                    # metadata: [':array', 'java.lang.String', '= "Answer: %g\\\\n"']
                    parsed_string = parse_string_value_from_parsed_metadata(self.parsed_metadata)
                    tokens.append(("value", parsed_string))
                else:
                    # matches the following sample
                    # opcode reg, 0xnum && (no metadata)
                    value = parse_hex_value(operand)
                    tokens.append(("value", value))

            elif 'ptr' in operand:
                # matches opcode reg *ptr* (with and without metadata)
                tokens.append(("address", operand))

            elif operand.startswith('[') and operand.endswith(']'):
                # matches opcode reg [address_calculation]
                tokens.append(("address", operand))

            else:
                tokens.append(("reg", operand))

        return tokens

    def tokenize_instruction(self):
        """
        determine the type of instruction and tokenize it accordingly
        currently follows the following format for tokenizing
                                each instruction [type]
                                /                \
                               /                  \
                              /                    \
                    function_call_instruction    normal_instruction
                        /      \                    /             \
                       /        \                  /               \
                      /          \                /                 \
                with_metadata   no_metadata   with_metadata      no_metadata
        """
        # list of tokens for this single instruction
        # each element is a tuple
        # tuple[0]: type of token: opcode, value, address, param, function(name of function)
        # tuple[1]: actual token
        instruction_tokens = list()
        opcode, operands, metadata = self.opcode, self.operands, self.metadata
        instruction_tokens.append(("opcode", opcode))
        if self.metadata:
            self.parsed_metadata = parse_metadata(self.metadata)
        if opcode == 'CALL':
            tokens = self.tokenize_function_call_instruction()
        else:
            tokens = self.tokenize_normal_instruction()

        instruction_tokens.extend(tokens)
        return instruction_tokens


def process_file(file_path: str):
    """
    extract metadata and functions from given source file (instructions.txt)
    """
    program = Program()  # instance of program for each file
    inside_function = False  # boolean to denote if we are inside function while parsing
    lines = []  # lines per function
    with open(file_path, 'r') as read_file:
        for line in read_file:
            line = line.strip('\n')
            if line.startswith('>>> FILE_START'):
                name = line.split(':')[1].strip()
                program.update_name(name)
            elif line.startswith('>>> FUNCTION_START'):
                inside_function = True
                function_address, function_name = line.split(':')[1].strip().split()
                addr_set_min = next(read_file).strip().split(':')[1].strip()
                addr_set_max = next(read_file).strip().split(':')[1].strip()
                function = Function(function_address, function_name, addr_set_min, addr_set_max)
            elif line.startswith('>>> FUNCTION_END'):
                function.add_lines(lines)
                program.add_function(function)
                inside_function = False
                lines = list()
            elif inside_function:
                lines.append(line)

    return program


def extract_tokens_and_save(_src_filepath: str, _dst_filepath: str):
    """
    Extract tokens from the source file (*instructions.txt) and saves them to the destination file
    (*instructions__tokens.txt)
    """
    print(f'tokenize {_src_filepath} -> {_dst_filepath}')

    program = process_file(_src_filepath)
    main_fn = program.get_function_by_name('main')
    program.stack.push(main_fn)
    # keep track of functions that are tokenized
    # to write them to the file
    tokenized_functions = []
    while not program.stack.is_empty():
        function = program.stack.pop()
        tokenized_functions.append(function)
        # tokenize will tokenize the function and return list of other functions(addresses)
        # that are being called by that function
        fn_list = function.tokenize_function()
        for fn_addr in fn_list:
            fn = program.functions[fn_addr]
            program.stack.push(fn)

    with open(_dst_filepath, 'w') as write_file:
        for function in tokenized_functions:
            write_file.write(f'function_name: {function.name}\n')
            write_file.write(str(function.tokens_nmt))
            write_file.write('\n\n')
            write_file.write('value\n')
            write_file.write(str(function.value_tokens))
            write_file.write('\n')
            write_file.write('address\n')
            write_file.write(str(function.address_tokens))
            write_file.write(f"\n\n")


def batch_process(instructions_root_dir: str,
                  dst_dir: str,
                  instructions_file: str = ''):
    def get_dst_filepath(_src_filepath):
        _dst_filepath = os.path.join(dst_dir, os.path.basename(_src_filepath))
        return os.path.splitext(_dst_filepath)[0] + '__tokens.txt'

    if instructions_file != '':  # process single instructions file...
        src_filepath = os.path.join(instructions_root_dir, instructions_file)
        if not os.path.isfile(src_filepath):
            raise Exception(f'ERROR: File not found: {src_filepath}')
        dst_filepath = get_dst_filepath(src_filepath)
        extract_tokens_and_save(src_filepath, dst_filepath)

    else:  # batch process all instructions in a directory...
        for subdir, dirs, files in os.walk(instructions_root_dir):
            for file in files:
                src_filepath = subdir + os.sep + file
                if src_filepath.endswith('-instructions.txt'):
                    dst_filepath = get_dst_filepath(src_filepath)
                    extract_tokens_and_save(src_filepath, dst_filepath)


def main_batch():
    parser = argparse.ArgumentParser()
    parser.add_argument('-I', '--instructions_root_dir',
                        help='specify the instructions root directory; required',
                        type=str,
                        required=True,
                        default='examples_ghidra_instructions/gcc')
    parser.add_argument('-i', '--instructions_file',
                        help='Optionally specify a specific Ghidra instructions file to process; '
                             'if left unspecified, then process whole instructions_root_dir directory',
                        type=str,
                        default='')
    parser.add_argument('-D', '--dst_dir',
                        help='directory where extracted token files will be saved',
                        type=str,
                        required=True,
                        default='examples_tokenized')

    args = parser.parse_args()

    # create destination root directory if does not already exist
    pathlib.Path(args.dst_dir).mkdir(parents=True, exist_ok=True)

    batch_process(instructions_root_dir=args.instructions_root_dir,
                  dst_dir=args.dst_dir,
                  instructions_file=args.instructions_file)


if __name__ == '__main__':
    main_batch()
