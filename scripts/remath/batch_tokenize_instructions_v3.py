"""
generate tokens from instructions.txt file that is generated by Ghidra
we need to extract tokens per function
we need to differentiate between function parameters and local variables
see: batch_tokenize_instructions.py for the previous version of the tokenizer
Follows the hierarchical nature of the program
1] program: object representation of instructions.txt file
2] program has multiple functions
3] function has metadata and multiple instructions
4] each instruction has opcode, operands and optionally metadata
5] each function has key-value pairs for:
    value tokens - local variables
    param tokens - function parameters / arguments
    address tokens - jump address
    memory tokens - memory address
    global function tokens - unique label for functions being called from main0
The tokenize function for a program calls the tokenize function for all of it's functions being called
from it's main function: each function in turn calls the tokenize function for its each instruction
instruction class tokenizes each instruction based on opcode, operands and metadata
"""

import argparse
import ast
import pathlib
import os
import re
from typing import List, Dict, Tuple, Union, Any
from collections import OrderedDict
import struct


# util functions for tokenization
def parse_unicode_string_list(ustr: str) -> List[str]:
    """
    extract unicode string literal from given unicode string
    """
    chunks = list()
    in_str = False
    str_start = None
    for i in range(1, len(ustr)):
        if ustr[i - 1] == 'u' and ustr[i] == '\'' and not in_str:
            in_str = True
            str_start = i + 1
        elif ustr[i - 1] != '\\' and ustr[i] == '\'' and in_str:
            in_str = False
            chunks.append(ustr[str_start: i])
    return chunks


def parse_metadata(metadata: str) -> List[str]:
    """
    parse metadata for each instruction (if any)
    returns: parsed metadata for array and unparsed metadata for others
    """
    if metadata.startswith('array'):
        parsed_metadata = [':array']
        t, rest = metadata[6:-1].split(',', maxsplit=1)
        parsed_metadata.append(t)

        elms = parse_unicode_string_list(rest.strip(' []'))
        for elm in elms:
            parsed_metadata.append(elm)

        return parsed_metadata
    else:
        return [':unparsed', metadata]


def parse_instruction_line(line) -> Tuple[str, List[str], str]:
    """
    parse each instruction line into opcode, operands, and metadata
    return opcode, operands, metadata
    """
    operands: List[str] = []
    metadata: str = ''
    if '>>>' in line:
        first, rest = line.split('>>>')
        line = first.strip(' ')
        metadata = rest.strip(' ')
    line = line.split(' ', maxsplit=1)
    opcode = line[0]
    if len(line) > 1:
        if ',' in line[1]:
            args = line[1].split(',')
            for arg in args:
                operands.append(arg)
        else:
            operands.append(line[1])
    return opcode, operands, metadata


def parse_fn_name_from_parsed_metadata(parsed_metadata: List[str]) -> Tuple[str, str]:
    if parsed_metadata[0] == ':array' and len(parsed_metadata) == 3:
        target_string = parsed_metadata[2]
        if '(' in target_string:
            fn_name = target_string.split('(')[0].split(' ')[-1]
            if "<" in fn_name:
                fn_name = fn_name.split("<")[0]
            return ':fn_name', fn_name
        else:
            raise Exception(f"ERROR parse_fn_name_from_parsed_metadata()\n"
                            f"target_string does not appear to contain a function signature"
                            f"target_string: {target_string}"
                            f"parsed_metadata: {parsed_metadata}")
    else:
        raise Exception(f"ERROR parse_fn_name_from_parsed_metadata()\n"
                        f"parsed_metadata is not an array of length 3:\n"
                        f"parsed_metadata: {parsed_metadata}")


def parse_hex_float_value(hex_value: str, size: str) -> str:
    # TODO: we need to handle other types
    # TODO: we need to be sure about the endianness
    if size == 'dword':
        return struct.unpack('!f', bytes.fromhex(hex_value))[0]
    elif size == 'qword':
        return struct.unpack('!d', bytes.fromhex(hex_value))[0]
    else:
        raise Exception(f'ERROR parse_hex_float_value(): Unhandled size {size}')


def parse_hex_value(value: str) -> Tuple[str, str, int]:
    """
    translate hex to decimal using 2's complement
    translate hex value ---------> binary -------------> decimal
    hex to binary: fixed mapping
    binary to decimal: 2's complement
    """
    original = value
    # need a dictionary that maps every float number into 4 bit binary number
    hex_to_bin = {"0": "0000", "1": "0001", "2": "0010", "3": "0011", "4": "0100",
                  "5": "0101", "6": "0110", "7": "0111", "8": "1000", "9": "1001",
                  "a": "1010", "b": "1011", "c": "1100", "d": "1101", "e": "1110",
                  "f": "1111"}
    result = ""
    # remove the initial 0x if it exists
    if value.startswith("-0x"):
        value = value[3:]
        result = -(int(value, 16))
        return ':interpreted_hex', original, result
    elif value.startswith("0x"):
        value = value[2:]
        if len(value) == 1:
            return ':interpreted_hex', original, int(value, 16)
    # convert to lower case
    value = value.lower()
    # for each digit convert it to binary
    for item in value:
        result += hex_to_bin[item]
    # if the initial value is zero: it's a positive number
    if result[0] == "0":
        # convert it to integer
        result = int(result, 2)
    # else the number is negative, convert it differently
    else:
        # flip the bits
        temp = ''.join(['1' if i == '0' else '0' for i in result])
        # add 1 and convert to decimal => convert to decimal and add 1
        result = -(int(temp, 2) + 1)
    return ':interpreted_hex', original, result


def param_or_local(memory_address: str) -> str:
    """
    take the memory address and find out if the memory address is used for parameter passing
    or for local variable
    return: "param" or "local"
    """
    if re.search('.*RBP \+ 0x.*', memory_address):
        return "param"
    else:
        return "local"


class Register:
    """
    Register classe: represents a register and its properties
    """

    def __init__(self, name, child_names=None):
        self.name = name
        self.defined_before = False
        if child_names:
            self.child_names = child_names
        else:
            self.child_names = []


class Stack:
    """
    class Stack: used to implement call stack of functions called from main function
    each program has an instance of Stack to keep track of the functions being called by the
    main function
    """

    def __init__(self):
        self.stack = []

    def push(self, item):
        self.stack.append(item)

    def pop(self):
        return self.stack.pop()

    def is_empty(self):
        return len(self.stack) == 0

    def len(self):
        return len(self.stack)


class Tokens:
    """
    class that represents key-value pair for different tokens (value, param, address)
    """

    def __init__(self, name: str, base: str):
        """
        name of token: value, param, address
        counter: start value for each token
        token_to_elm: ordered dict for token to element
        """
        self.name = name
        self.base = base
        self.counter = 0
        # key value pair from key to value
        self.token_to_elm: Dict[str, str] = OrderedDict()
        # reverse dictionary of self.elm_to_token: for efficient search to find if elm exists already
        self.elm_to_token: Dict[str, str] = dict()

    def add_token(self, elm: str) -> str:
        """
        elm: element to add to the token dict
        return: the key to which the elm was assigned to or the previous key if
        the element already exists: same id for repeating tokens
        """
        # if the elm already exists in the dict, just return the key of that element
        if elm in self.elm_to_token:
            return self.elm_to_token[elm]
        # if not assign the elm to new key and return new key
        key = self.base + str(self.counter)
        # update key-elm dict
        self.token_to_elm[key] = elm
        # update elm-key dict
        self.elm_to_token[elm] = key
        self.counter += 1
        return key

    def get_key(self, elm: str):
        """
        get key for elements already in the token
        """
        if elm in self.elm_to_token:
            return self.elm_to_token[elm]
        else:
            raise Exception(f'Unable to find element {elm} in the tokenmap: {self.name}')

    def __str__(self) -> str:
        """
        string representaton of the key value pairs
        to write them to a __tokens.txt file
        """
        return ''.join(str(key) + ':' + str(value) + '\n' for key, value in self.token_to_elm.items())


class Program:
    """
    Program class: holds multiple functions: equivalent to the object representation of
    given instructions.txt file
    """

    def __init__(self):
        # file_name: name of the file that represents the program
        self.name: str = ''
        # dict of functions [address:function]
        # [name:function] is not valid because we can have multiple functions with the same name [printf]
        # this means we should use address for call navigation
        self.functions: Dict[str, Function] = dict()
        # book-keeping for unique functions [name:address]
        self.name_to_address: Dict[str, str] = dict()
        # initialize it's call stack of functions being called from it's main function
        self.stack = Stack()
        # each program has global function_tokens_map to give unique labels to all the
        # functions called from main: updated from function.tokenize()
        self.function_tokens_map = Tokens(name="function", base="_f")
        # keep track of jump [conditional/unconditional] flags
        self.jump_flags = ['JMP', 'JO', 'JNO', 'JS', 'JNS', 'JE', 'JZ', 'JNE', 'JNZ', 'JB',
                           'JNAE', 'JC', 'JNB', 'JAE', 'JNC', 'JBE', 'JNA', 'JA', 'JNBE',
                           'JL', 'JNGE', 'JGE', 'JNL', 'JLE', 'JNG', 'JG', 'JNLE', 'JP',
                           'JPE', 'JNP', 'JPO', 'JCXZ', 'JECXZ', 'JRCXZ']

        # list of global addresses
        # this will be used by each instruction to determine if each address is either global or not
        # and return appropriate information to the function which will do the update to the
        # global_tokens_map
        self.globals = []
        # global tokens map: key-value pair for global tokens
        # note this is being updated by a function.tokenize(): when it tokenizes each
        # instruction and determines that the address is a global address, it returns appropriate
        # information to the function and function updates global_tokens_map
        self.global_tokens_map = Tokens(name="globals", base='_g')

        # list of library functions: so that we don't try to tokenize them
        self.library_functions = ['printf', 'sqrt', 'sin', 'abs', 'fmin', 'fmax']

    def update_name(self, name: str) -> None:
        self.name = name

    def add_function(self, function: "Function") -> None:
        if function.address in self.functions:
            raise Exception(f"[ERROR]: function with address: {function.address} already exists!")
        self.functions[function.address] = function
        # add book-keeping
        if function.name not in self.name_to_address:
            self.name_to_address[function.name] = function.address

    def get_function_by_name(self, name: str) -> "Function":
        if name not in self.name_to_address:
            raise Exception(f"[ERROR]: unable to find function with name: {name}")
        address = self.name_to_address[name]
        return self.functions[address]

    def add_globals(self, global_addresses):
        """
        add list of global addresses to the current program
        """
        self.globals.extend(global_addresses)


class Function:
    """
    Function class: holds the data for each function in the instructions.txt file
    Program has multiple functions
    """

    def __init__(self, address: str, name: str, min_address: str, max_address: str):
        self.name: str = name
        self.address: str = address
        self.min_address: str = min_address
        self.max_address: str = max_address
        # raw lines for the function [not tokenized yet]
        self.lines: List[str] = list()
        # tokenized version of lines: List of instructions
        self.instructions: List[Instruction] = list()
        # list of tokens to be used for NMT: no <sep> tag
        self.token_sequence = list()
        # list of tokens to be used for NMT: with <sep> tag
        self.token_sequence_sep = list()
        # each function has key-value pairs for value tokens, param tokens, and memory_address
        # and instruction_address tokens [for jump targets]
        self.value_tokens_map = Tokens(name="value", base='_v')
        self.param_tokens_map = Tokens(name="param", base='_p')
        self.memory_address_tokens_map = Tokens(name="memory_address", base='_m')
        # also tokenize the jmp [conditional/unconditional] targets
        self.jump_address_tokens_map = Tokens(name="jump_address", base='_a')
        # list of functions called by this function
        self.called_fns = []
        # The function has parameters: which are passed either through the registers or throuhg the
        # stack: if passed through stack, they have [RBP + positive_offset] while local variables have
        # [RBP + negative_offset]
        # keep track of registers used for passing arguments: if they are used (are in dest) before any
        # values are moved to them first: they are parameters
        # registers used to pass the parameters
        # defined here: because they also hold a state: if they are defined or not: which is
        # specific for a given function
        self.param_registers = {'EDI': Register('EDI'),
                                'ESI': Register('ESI'),
                                'EDX': Register('EDX'),
                                'ECX': Register('ECX'),
                                'R8D': Register('R8D'),
                                'R9D': Register('R9D'),
                                'RDI': Register('RDI', child_names=['EDI']),
                                'RSI': Register('RSI', child_names=['ESI']),
                                'RDX': Register('RDX', child_names=['EDX']),
                                'RCX': Register('RCX', child_names=['ECX']),
                                'R8': Register('R8', child_names=['R8D']),
                                'R9': Register('R9', child_names=['R9D']),
                                'XMM0': Register('XMM0'),
                                'XMM1': Register('XMM1'),
                                'XMM2': Register('XMM2'),
                                'XMM3': Register('XMM3'),
                                'XMM4': Register('XMM4'),
                                'XMM5': Register('XMM5'),
                                'XMM6': Register('XMM6'),
                                'XMM7': Register('XMM7')}

        # keep track of jump target addresses
        self.jump_targets = []

        # each function has nmt_tokens: each nmt_token is associated with an instruction_address
        # track address for each token: no <sep> token
        self.address_sequence = []
        # track address for each token: with <sep> token: for <sep> token: put the next address
        self.address_sequence_sep = []

        # certain opcodes sets the values of other registers:
        # need a clear way to handle this: for example cdq commands sets some bits in the
        # edx register
        self.register_setter_opcodes = {'CDQ': ['EDX']}

    def update_name(self, new_name):
        self.name = new_name

    def add_lines(self, lines: List[str]) -> None:
        """
        add raw lines for the given function
        """
        self.lines.extend(lines)

    def instruction_address_handler(self, token: str, address: str,
                                    nmt_tokens_instruction: List[str],
                                    nmt_sequence_address: List[str]) -> None:
        """
        handles adding instruction address to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        """
        # add instruction_address to the nmt_tokens if only it's the
        # target of the conditional and unconditional jump
        if token in self.jump_targets:
            key = self.jump_address_tokens_map.add_token(token)
            # because it's a jump target append 'TAG' token
            nmt_tokens_instruction.append('TAG')
            nmt_tokens_instruction.append(key)
            # address for TAG
            nmt_sequence_address.append(address)
            # address for _a#
            nmt_sequence_address.append(address)

    def jump_address_handler(self, token: str, address: str,
                             nmt_tokens_instruction: List[str],
                             nmt_sequence_address: List[str]) -> None:
        """
        handles adding jump address to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        """
        # replace jmp 0xabcd => jmp _a0
        key = self.jump_address_tokens_map.add_token(token)
        nmt_tokens_instruction.append(key)
        nmt_sequence_address.append(address)

    def value_handler(self, token: str, address: str,
                      nmt_tokens_instruction: List[str],
                      nmt_sequence_address: List[str]) -> None:
        """
        handles adding jump values to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        """
        key = self.value_tokens_map.add_token(token)
        nmt_tokens_instruction.append(key)
        nmt_sequence_address.append(address)

    def memory_address_handler(self, token: str, address: str, index: int,
                               nmt_tokens_instruction: List[str],
                               nmt_sequence_address: List[str]) -> None:
        """
        handles adding jump values to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        memory address can be either param token or address token
        based on offset from RBP
        check if the memory location is a param or a local variable
        instruction_address, opcode, dst_register, src_mem_location
        """
        if index == 3:
            address_type = param_or_local(token)
            # address_type is either param or local
            if address_type == "param":
                key = self.param_tokens_map.add_token(token)
                nmt_tokens_instruction.append(key)
                nmt_sequence_address.append(address)
            else:
                key = self.memory_address_tokens_map.add_token(token)
                nmt_tokens_instruction.append(key)
                nmt_sequence_address.append(address)
        else:
            key = self.memory_address_tokens_map.add_token(token)
            nmt_tokens_instruction.append(key)
            nmt_sequence_address.append(address)

    @staticmethod
    def function_name_handler(current_fn_name: str, address: str,
                              function_tokens_map: "Tokens",
                              library_functions: List[str],
                              nmt_tokens_instruction: List[str],
                              nmt_sequence_address: List[str]) -> None:
        """
        handles adding jump values to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        """
        # for function names in library functions: treat them as primitives
        if current_fn_name not in library_functions:
            label = function_tokens_map.add_token(current_fn_name)
            nmt_tokens_instruction.append(label)
        else:
            nmt_tokens_instruction.append(current_fn_name)
        nmt_sequence_address.append(address)

    def register_handler(self, token: str, address: str, index: int,
                         nmt_tokens_instruction: List[str],
                         nmt_sequence_address: List[str]) -> None:
        """
        handles adding jump values to required token fields and updates them
        also updates nmt token list and nmt address sequence list
        instruction_tokens: [address, opcode, dst, src]
        index 3: represents that the register is in src location
        not on the dst location
        """
        if index == 3 and token in self.param_registers:
            if not self.param_registers[token].defined_before:
                key = self.param_tokens_map.add_token(token)
                nmt_tokens_instruction.append(key)
                nmt_sequence_address.append(address)
                self.param_registers[token].defined_before = True
        else:
            nmt_tokens_instruction.append(token)
            nmt_sequence_address.append(address)
        # if the param_registers appear in the src then set defined_before=True
        if index == 2 and token in self.param_registers:
            self.param_registers[token].defined_before = True
            # also set it's childs to be defined as well recursively
            # example rdx is defined: edx should be defined as well
            child_names = self.param_registers[token].child_names
            for child_name in child_names:
                self.param_registers[child_name].defined_before = True

    def tokenize_function(self, function_tokens_map: "Tokens", jump_flags: List[str],
                          global_addresses: List[str], global_tokens_map: "Tokens",
                          library_functions: List[str]) -> List[str]:
        """
        convert the raw lines (self.lines) into Instruction
        function_token: global FunctionToken instance of a program to give unique labels to each function in the
        instructions.txt file
        and call tokenize instructions that tokenizes them and updates tokens_nmt
        also return a list of functions called by this function
        """
        # list of functions called by this function
        functions_called = list()
        for line in self.lines:
            addr_str, rest = line.split('::')
            addr = addr_str.strip(' ')
            opcode, operands, metadata = parse_instruction_line(rest.strip(' '))
            instruction = Instruction(addr, opcode, operands, metadata)
            self.instructions.append(instruction)
        # a function has multiple instruction
        # call tokenize_instruction for each instruction to tokenize them all
        for instruction in self.instructions:
            # each instruction needs to be tokenized according to it's type
            # call instruction.tokenize() that detects the type and tokenizes accordingly
            # this will also update the tokenized list for that instruction
            instruction.tokenize_instruction(jump_flags, global_addresses)

        # find jump targets and update the jump_targets list
        # jump tags
        for instruction in self.instructions:
            for token_type, token in instruction.tokenized:
                if token_type == 'jump_address':
                    self.jump_targets.append(token)

        for instruction_index, instruction in enumerate(self.instructions):
            # list of tokens for nmt for each instruction
            nmt_tokens_instruction = list()
            # list of address for each token: each time we append tokens
            # we need to append the address
            nmt_sequence_address = list()
            # keep track of current function name: to find if it's printf or not
            for index, instruction_token in enumerate(instruction.tokenized):
                token_type, token = instruction_token
                if token_type == "instruction_address":
                    self.instruction_address_handler(token, instruction.address,
                                                     nmt_tokens_instruction,
                                                     nmt_sequence_address)
                elif token_type == "jump_address":
                    self.jump_address_handler(token, instruction.address,
                                              nmt_tokens_instruction,
                                              nmt_sequence_address)
                elif token_type == "value":
                    self.value_handler(token, instruction.address,
                                       nmt_tokens_instruction,
                                       nmt_sequence_address)
                elif token_type == "memory_address":
                    self.memory_address_handler(token, instruction.address, index,
                                                nmt_tokens_instruction,
                                                nmt_sequence_address)
                elif token_type == 'function_name':
                    self.function_name_handler(token, instruction.address,
                                               function_tokens_map,
                                               library_functions,
                                               nmt_tokens_instruction,
                                               nmt_sequence_address)
                elif token_type == 'function_address':
                    functions_called.append(token)
                elif token_type == "register":
                    self.register_handler(token, instruction.address, index,
                                          nmt_tokens_instruction,
                                          nmt_sequence_address)
                elif token_type == "global":
                    key = global_tokens_map.add_token(token)
                    nmt_tokens_instruction.append(key)
                    nmt_sequence_address.append(instruction.address)
                elif token_type == "opcode":
                    opcode = token
                    if opcode in self.register_setter_opcodes:
                        for _, registers in self.register_setter_opcodes.items():
                            for register in registers:
                                self.param_registers[register].defined_before = True
                    nmt_tokens_instruction.append(token)
                    nmt_sequence_address.append(instruction.address)
                else:
                    raise Exception(f"Unhandled token type: {token_type}")

            # add the tokens of a line (instructon) to tokens_nmt
            # add to the token_sequence and address_sequence no <sep>
            self.token_sequence.extend(nmt_tokens_instruction)
            self.address_sequence.extend(nmt_sequence_address)
            # add the tokens of a line (instruction) to tokens_nmt
            # add the <sep> tag for separating instructions
            if instruction_index > 0:
                # add <sep> token
                self.token_sequence_sep.append('<sep>')
                # add address for <sep> token
                self.address_sequence_sep.append(instruction.address)

            # add token_sequence and address_sequence with <sep>
            self.token_sequence_sep.extend(nmt_tokens_instruction)
            self.address_sequence_sep.extend(nmt_sequence_address)

        assert len(self.token_sequence) == len(self.address_sequence)
        assert len(self.token_sequence_sep) == len(self.address_sequence_sep)
        return functions_called


class Instruction:
    """
    Instruction class: each line inside the function in the instructions.txt file
    Function has multiple instructions
    """

    def __init__(self, address: str, opcode: str, operands: List[str], metadata: str):
        """
        each instruction has address [in memory], and tuple (opcode, operands)
        some instructions have metadata
        parent is the function this instruction belongs to
        """
        self.address: str = address
        self.opcode: str = opcode
        self.operands: List[str] = operands
        self.metadata: str = metadata
        self.parsed_metadata = []
        # tokenized version of me
        self.tokenized = []

    def get_info_from_string(self):
        """
        extract information from string format
        """
        val = self.parsed_metadata[2]
        # format strings in a nice way
        # remove = character if any
        val = re.sub('= ', '', val)
        # remove " characters if any
        val = re.sub('"', '', val)
        # remove any 1 or more \ followed by n
        val = re.sub(r'\\+n', '', val)
        # uninitialized globals have '??' in their string representation
        if '??' in val:
            return ':string', 'not initialized'
        else:
            return ':string', val

    def get_info_from_hex(self, size):
        # check if floating point registers present in the operands
        hex_str = self.parsed_metadata[2][2:-1]
        temp_str = hex_str.lstrip("0")
        if len(temp_str) <= 2:
            hex_str = '0x' + temp_str
            return parse_hex_value(hex_str)
        return ':interpreted_hex_float', hex_str, parse_hex_float_value(hex_str, size)

    def get_info_from_parsed_metadata(self, size=None) -> \
            Union[Tuple[str, str, str], Tuple[str, str], Tuple[str, str, int]]:
        # interpreted hex float
        if len(self.parsed_metadata) == 3 and self.parsed_metadata[2].startswith('= ') and \
                self.parsed_metadata[2][-1] == 'h':
            return self.get_info_from_hex(size)
        # string
        elif len(self.parsed_metadata) == 3 and self.parsed_metadata[0] == ':array' and \
                self.parsed_metadata[1] == 'java.lang.String':
            return self.get_info_from_string()
        else:
            raise Exception(f"Unable to handle this parsed metadata: {self.parsed_metadata}")

    def tokenize_function_call_instruction(self) -> List[Tuple[str, str]]:
        """
        tokenize operands for function call (with metadata / no metadata)
        returns a list of tuples
        """
        # return list is a tuple
        # tuple [0]: function_name, function_address
        # tuple[1]: actual token
        return_list = []
        if self.parsed_metadata:
            fn_name = parse_fn_name_from_parsed_metadata(self.parsed_metadata)
            # also return the address of the function: because it's uinque instead of names
            return_list.append(("function_name", fn_name[1]))
        else:
            raise Exception(f"Unable to get function name from metadata!")
        # call followed by 0xaddress, gives the address of the function
        if self.opcode == 'CALL' and self.operands[0].startswith('0x'):
            return_list.append(("function_address", self.operands[0]))

        return return_list

    def tokenize_jump_instruction(self) -> List[Tuple[str, str]]:
        """
        tokenize jump instruction: we don't want to convert the hex operand into decimal
        instead we want to return it as a jump address
        """
        assert len(self.operands) == 1
        return [("jump_address", self.operands[0])]

    def value_token_handler(self, operand: str, tokens) -> None:
        """
        handle instruction that starts with 0x or -0x
        """
        if self.parsed_metadata:
            # matches following sample
            # opcode reg, 0xnum
            # metadata: [':array', 'java.lang.String', '= "Answer: %g\\\\n"']
            parsed_value = self.get_info_from_parsed_metadata()
            tokens.append(("value", parsed_value))
        else:
            # matches the following sample
            # opcode reg, 0xnum && (no metadata)
            value = parse_hex_value(operand)
            tokens.append(("value", value))

    def global_token_handler(self, operand, tokens):
        """
        find if memory address is global memory address and handle it
        """
        size = operand.split(' ')[0]
        parsed_value = self.get_info_from_parsed_metadata(size)
        tokens.append(("global", parsed_value))

    @staticmethod
    def check_global(operand, global_addresses):
        """
        check if memory address is a global
        return True if global mem address else False
        """
        match = re.search(r'\[.*', operand)
        address = match.group(0)[1:-1]
        if address in global_addresses:
            return True
        else:
            return False

    def normal_memory_ptr_token_handler(self, operand, tokens):
        """
        normal memory access [not global] with ptr in it
        """
        # matches opcode reg *ptr* (with and without metadata)
        if self.parsed_metadata:
            # get size directive
            size = operand.split(' ')[0]
            parsed_value = self.get_info_from_parsed_metadata(size)
            tokens.append(("value", parsed_value))
        else:
            tokens.append(("memory_address", operand))

    def normal_memory_token_handler(self, operand, tokens):
        """
        normal memory access [not global] no ptr in it
        """
        # matches opcode reg [address_calculation]
        # sometimes has reg [address] metadata that has Answer: %d
        if self.parsed_metadata:
            parsed_value = self.get_info_from_parsed_metadata()
            tokens.append(("value", parsed_value))
        else:
            tokens.append(("memory_address", operand))

    def memory_address_ptr_handler(self, operand: str, global_addresses, tokens) -> None:
        """
        handle instruction [memory access] that has ptr in it
        """
        is_global = self.check_global(operand, global_addresses)
        if is_global:
            # global variable treat differently
            self.global_token_handler(operand, tokens)
        else:
            # normal memory access with ptr in it
            self.normal_memory_ptr_token_handler(operand, tokens)

    def memory_address_handler(self, operand, global_addresses, tokens):
        """
        handle instruction [memory access] but not ptr in it
        """
        is_global = self.check_global(operand, global_addresses)
        if is_global:
            # global variable treat differently
            self.global_token_handler(operand, tokens)
        else:
            # normal memory access with ptr in it
            self.normal_memory_token_handler(operand, tokens)

    def tokenize_normal_instruction(self, global_addresses: List[str]) -> List[Tuple[str, Any]]:
        """
        tokenize normal operands (with metadata / no metadata)
        """
        tokens = list()
        for operand in self.operands:
            # value tokens
            if operand.startswith('0x') or operand.startswith('-0x'):
                self.value_token_handler(operand, tokens)
            # memory access tokens with ptr in instruction
            elif 'ptr' in operand:
                self.memory_address_ptr_handler(operand, global_addresses, tokens)
            # memory access tokens without ptr in instruction
            elif operand.startswith('[') and operand.endswith(']'):
                self.memory_address_handler(operand, global_addresses, tokens)
            # remaining: registers
            else:
                tokens.append(("register", operand))
        return tokens

    def tokenize_instruction(self, jump_flags: List[str], global_addresses: List[str]) -> None:
        """
        determine the type of instruction and tokenize it accordingly
        currently follows the following format for tokenizing
                                each instruction [type: division for parsing]
                                /                     |          \
                               /                      |           \
                              /                       |            \
                    function_call_instruction     jump inst       normal_instruction
                        /      \                                    /             \
                       /        \                                  /               \
                      /          \                                /                 \
                with_metadata   no_metadata                     with_metadata      no_metadata
        save the tokenized form in it's tokenized list
        """
        self.tokenized.append(("instruction_address", self.address))
        self.tokenized.append(("opcode", self.opcode))
        if self.metadata:
            self.parsed_metadata = parse_metadata(self.metadata)
        if self.opcode == 'CALL':
            tokens = self.tokenize_function_call_instruction()
        elif self.opcode in jump_flags:
            tokens = self.tokenize_jump_instruction()
        else:
            tokens = self.tokenize_normal_instruction(global_addresses)

        self.tokenized.extend(tokens)


def process_file(file_path: str) -> "Program":
    """
    extract metadata and functions from given source file (instructions.txt)
    """
    program = Program()  # instance of program for each file
    inside_function = False  # boolean to denote if we are inside function while parsing
    lines = []  # lines per function
    with open(file_path, 'r') as read_file:
        for line in read_file:
            line = line.strip('\n')
            if line.startswith('>>> FILE_START'):
                name = line.split(':')[1].strip()
                program.update_name(name)
            elif line.startswith('>>> GLOBALS'):
                global_addresses = line.split(':')[-1].strip()
                global_addresses = ast.literal_eval(global_addresses)
                program.add_globals(global_addresses)
            elif line.startswith('>>> FUNCTION_START'):
                inside_function = True
                function_address, function_name = line.split(':')[1].strip().split()
                # Note the function address are without initial 0x
                # I think it's better to put the initial 0x for address
                # add 0x to the function address
                function_address = '0x' + function_address
                addr_set_min = next(read_file).strip().split(':')[1].strip()
                addr_set_max = next(read_file).strip().split(':')[1].strip()
                function = Function(function_address, function_name, addr_set_min, addr_set_max)
            elif line.startswith('>>> FUNCTION_END'):
                function.add_lines(lines)
                program.add_function(function)
                inside_function = False
                lines = list()
            elif inside_function:
                lines.append(line)

    return program


def extract_tokens_and_save(_src_filepath: str, _dst_filepath: str) -> None:
    """
    Extract tokens from the source file (*instructions.txt) and saves them to the destination file
    (*instructions__tokens.txt)
    """
    print(f'tokenize {_src_filepath} -> {_dst_filepath}')

    program = process_file(_src_filepath)
    main_fn = program.get_function_by_name('main')
    program.stack.push(main_fn)
    # keep track of functions that are tokenized
    # to write them to the file
    tokenized_functions = []
    while not program.stack.is_empty():
        function = program.stack.pop()
        if function not in tokenized_functions:
            tokenized_functions.append(function)
            # tokenize will tokenize the function and return list of other functions(addresses)
            # that are being called by that function
            fn_list = function.tokenize_function(program.function_tokens_map, program.jump_flags,
                                                 program.globals, program.global_tokens_map,
                                                 program.library_functions)
            for fn_addr in fn_list:
                fn = program.functions[fn_addr]
                # do not push library functions
                # filter some functions that have "<...>" in their signature
                if "<" in fn.name:
                    new_name = fn.name.split("<")[0]
                    fn.update_name(new_name)
                if fn.name not in program.library_functions:
                    # also if the function is tokenized already: don't tokenize again
                    if fn not in tokenized_functions:
                        program.stack.push(fn)

    # collect some stats about the program and dump them to the end of __tokens.txt file

    with open(_dst_filepath, 'w') as write_file:
        # write global information about program
        write_file.write(f'function_tokens_map\n')
        write_file.write(str(program.function_tokens_map))
        write_file.write('\n\n')

        # write globals
        write_file.write(f'global_tokens_map\n')
        write_file.write(str(program.global_tokens_map))
        write_file.write('\n\n')

        for function in tokenized_functions:
            write_file.write(f'function_name: {function.name}\n')
            if function.name != 'main':
                try:
                    write_file.write(f'function_label: {program.function_tokens_map.get_key(function.name)}\n')
                except:
                    breakpoint()

            # token sequence and sequence address without <sep>
            write_file.write(f'token_sequence\n')
            write_file.write(str(function.token_sequence))
            write_file.write('\n\n')
            # write_file.write('address_sequence\n')
            # write_file.write(str(function.address_sequence))
            # write_file.write('\n\n')

            # token sequence and sequence address with <sep>
            # write_file.write(f'token_sequence_sep\n')
            # write_file.write(str(function.token_sequence_sep))
            # write_file.write('\n\n')
            # write_file.write('address_sequence_sep\n')
            # write_file.write(str(function.address_sequence_sep))
            # write_file.write('\n\n')

            write_file.write('address_tokens_map\n')
            write_file.write(str(function.jump_address_tokens_map))
            write_file.write('\n\n')
            write_file.write('param_tokens_map\n')
            write_file.write(str(function.param_tokens_map))
            write_file.write('\n\n')
            write_file.write('value_tokens_map\n')
            write_file.write(str(function.value_tokens_map))
            write_file.write('\n\n')
            write_file.write('memory_address_tokens_map\n')
            write_file.write(str(function.memory_address_tokens_map))
            write_file.write(f"\n\n")


def batch_process(instructions_root_dir: str,
                  dst_dir: str,
                  instructions_file: str = '') -> None:
    def get_dst_filepath(_src_filepath):
        _dst_filepath = os.path.join(dst_dir, os.path.basename(_src_filepath))
        return os.path.splitext(_dst_filepath)[0] + '__tokens.txt'

    if instructions_file != '':  # process single instructions file...
        src_filepath = os.path.join(instructions_root_dir, instructions_file)
        if not os.path.isfile(src_filepath):
            raise Exception(f'ERROR: File not found: {src_filepath}')
        dst_filepath = get_dst_filepath(src_filepath)
        extract_tokens_and_save(src_filepath, dst_filepath)

    else:  # batch process all instructions in a directory...
        for subdir, dirs, files in os.walk(instructions_root_dir):
            for file in files:
                src_filepath = subdir + os.sep + file
                if src_filepath.endswith('-instructions.txt'):
                    dst_filepath = get_dst_filepath(src_filepath)
                    extract_tokens_and_save(src_filepath, dst_filepath)


def main_batch() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument('-I', '--instructions_root_dir',
                        help='specify the instructions root directory; required',
                        type=str,
                        required=True,
                        default='examples_ghidra_instructions/gcc')
    parser.add_argument('-i', '--instructions_file',
                        help='Optionally specify a specific Ghidra instructions file to process; '
                             'if left unspecified, then process whole instructions_root_dir directory',
                        type=str,
                        default='')
    parser.add_argument('-D', '--dst_dir',
                        help='directory where extracted token files will be saved',
                        type=str,
                        required=True,
                        default='examples_tokenized')

    args = parser.parse_args()

    # create destination root directory if does not already exist
    pathlib.Path(args.dst_dir).mkdir(parents=True, exist_ok=True)

    batch_process(instructions_root_dir=args.instructions_root_dir,
                  dst_dir=args.dst_dir,
                  instructions_file=args.instructions_file)


if __name__ == '__main__':
    main_batch()
