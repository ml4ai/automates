ReaderType = "TextEngine" // "TextEngine", "MarkdownEngine", "CommentEngine"

TextEngine {
  // Override the default values here
//  basePath = /org/clulab/aske_automates
//  masterRulesPath = ${OdinEngine.basePath}/grammars/master.yml
//  entityRulesPath = ${OdinEngine.basePath}/grammars/entities/grammar/entities.yml
//  avoidRulesPath = ${OdinEngine.basePath}/grammars/avoidLocal.yml
//  taxonomyPath = ${OdinEngine.basePath}/grammars/taxonomy.yml
//  maxHops = 15

//  documentFilter = "length"
  preprocessorType = "Light" // "PassThrough" is for the webapp and markdown , "Light" (get rid of non-language looking strings and extra long words), "EdgeCase" for docs that have both prose and tables of contents and such
  enableExpansion = true
  validArgs = ["description"] #which args are to be expanded
  freqWordsPath = "./src/main/resources/frequentWords.tsv"


  entityFinder {
    enabled = true
    finderTypes = ["rulebased", "gazetteer"]

    // Rule-based
    entityRulesPath = ${TextEngine.basePath}/grammars/entities/grammar/entities.yml
    avoidRulesPath = ${TextEngine.basePath}/grammars/entities/grammar/avoid.yml
    maxHops = 15

    // grobid-quantities
    taxonomy = ${TextEngine.taxonomyPath}
    domain = "localhost"
    port = "8060"

    // Gazetteer
    lexicons = ["unit.tsv", "GreekLetter.tsv", "model.tsv"]
  }


}

MarkdownEngine {

  masterRulesPath = ${TextEngine.basePath}/grammars/markdown/master.yml
  taxonomyPath = ${TextEngine.taxonomyPath}
  preprocessorType = "PassThrough" // keep everything---markdown should be reasonably clean
  enableExpansion = true
  validArgs = ["description"] #which args are to be expanded
  freqWordsPath = "./src/main/resources/frequentWords.tsv"


  entityFinder {
    enabled = true
    finderTypes = ["rulebased", "gazetteer"]

    // Rule-based
    entityRulesPath = ${TextEngine.basePath}/grammars/entities/grammar/entities.yml
    avoidRulesPath = ${TextEngine.basePath}/grammars/entities/grammar/avoid.yml
    maxHops = 15

    // grobid-quantities
    taxonomy = ${TextEngine.taxonomyPath}
    domain = "localhost"
    port = "8060"

    // Gazetteer
    lexicons = ["unit.tsv", "GreekLetter.tsv", "model.tsv"]
  }


}

CommentEngine {
  masterRulesPath = ${TextEngine.basePath}/grammars/comments/master.yml
  taxonomyPath = ${TextEngine.taxonomyPath}

  enableLexiconNER = true
  enableExpansion = false
  validArgs = ${TextEngine.validArgs}
  preprocessorType = "EdgeCase"
  documentFilter = "length"
  freqWordsPath = ${TextEngine.freqWordsPath}

  entityFinder {
    enabled = true
    finderTypes = ["grfn"]

    // GrFN-based string match
    grfnFile = ${apps.grfnFile}

  }
}


apps {

  projectDir = "/automates"
  //grfnFile = "path/to/PETPT_GrFN.json"
  inputDirectory = ""
  outputDirectory = ""
  includeAnnotationField = true // have the mentionAssemblyJson contain a field for easy annotation; filled by default with 0s (for likely incorrect extractions) and 1s (for likely correct extractions)
  inputType = "json" // "md"

  predictedEquations = "/local/path/to/PETPT_equations.txt"

  numAlignments = 3
  numAlignmentsSrcToComment = 3
  scoreThreshold = 0.0
  maxSVOgroundingsPerVar = 5
  groundToSVO = false
  groundToWiki = false
  saveWikiGroundingsDefault = false
  pathToWikiGroundings = ""
  numOfWikiGroundings = 3
  appendToGrFN = true
  debug = true
  commentTextAlignmentScoreThreshold = -1.0 //todo: change if the scoring function changes
  serializerName = "AutomatesJSONSerializer" // another option - JSONSerializer: there is a small difference between the two serializers, so the same serializer has to be used to serialize and deserialize mentions


  exportAs =  ["tsv", "json"] // also "serialized"; no other formats yet specified, but can be added!
  loadMentions = true
  mentionsFile = "/local/path/to/PT-mentions.json"
  appendToGrFN = true
  //=====AlignmentBaselineData=====
  baselineDir = "./input/LREC/dev"
  baselineOutputDirectory = "./output/LRECBaseline"
  pdfalignDir = ${apps.projectDir}/automates/apps/pdfalign
  baselineTextInputDirectory = ${apps.baselineDir}/ParsedJsons
  baselineEquationDir = ${apps.baselineDir}/equations
  baselineGoldDir = ${apps.baselineDir}/gold
  baselineAlignedLatexDir = ${apps.baselineDir}/latex_alignments
  eqnPredFile = ${apps.baselineDir}/equations/equationsFromTranslator.txt
  eqnSrcFile = ${apps.baselineDir}/equations/croppedImages.txt
  exportedMentionsDir = ${apps.baselineDir}/extractedMentions
}

alignment {
  alignerType = "pairwisew2v"
  w2vPath = ${apps.projectDir}/automates/text_reading/src/main/resources/vectors.txt // a word embeddings file (e.g., GloVe) with this header: <len_of_vocab><space><embedding_size>, e.g., "6B 50" (no quotes); use of different sets of embeddings will require adjusting alignment similarity thresholds, e.g., commentTextAlignmentScoreThreshold
  relevantArgs = ["variable", "description"] // also possible ["variable"]
}

// if no other changes come to the model comparison alignment, can remove - tbd
modelComparisonAlignment {
  alignerType = "pairwisew2v"
  w2vPath = ${apps.projectDir}/automates/text_reading/src/main/resources/vectors.txt
  relevantArgs = ["description"] // also possible ["variable"]
}

grounding {
  WikiCacheFilePath = "./src/main/resources/wikidata-cache.json"
  sparqlDir = ${apps.projectDir}/automates/text_reading/sparql
}